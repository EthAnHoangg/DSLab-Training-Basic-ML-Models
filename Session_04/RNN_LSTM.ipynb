{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3972,"status":"ok","timestamp":1679629253647,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"cpHVllrFM2pV","outputId":"8036c9f8-f410-44c1-ff85-b32caef08fc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679629253648,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"FudrvRccZCI4","outputId":"46dd817a-d0d0-4237-ad2c-4f6b3aec18de"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679629253649,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"iJUGddoSYiXg","outputId":"391eb596-f104-45fd-8344-04ec7af60f69"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/RNN\n"]}],"source":["cd drive/MyDrive/RNN"]},{"cell_type":"markdown","metadata":{"id":"FtZAAunNMCK_"},"source":["# Import necessary libs"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4442,"status":"ok","timestamp":1679629258086,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"HlG1TNrRMCLD","outputId":"5b99ffdc-4d7f-4e93-9853-bbb2268ee799"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["import numpy as np\n","import torch\n","import os\n","from collections import defaultdict\n","import re\n","import random\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","tf.disable_eager_execution()\n"]},{"cell_type":"markdown","metadata":{"id":"F51sgM0FbCtx"},"source":["# DataReader"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679629258087,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"MXDqKxsiZvxk"},"outputs":[],"source":["import numpy as np\n","import random\n","\n","class DataReader:\n","  def __init__(self, data_path, batch_size):\n","    self._batch_size = batch_size\n","    with open(data_path) as f:\n","      d_lines = f.read().splitlines()\n","\n","    self._data = []\n","    self._labels = []\n","    self._sentence_lengths = []\n","    self._final_tokens = [] \n","    for data_id, line in enumerate(d_lines):\n","      features = line.split('<fff>')\n","      label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n","      tokens = features[3].split()\n","\n","      self._data.append(tokens)\n","      self._sentence_lengths.append(sentence_length)\n","      self._labels.append(label)\n","      self._final_tokens.append(tokens[-1])\n","\n","    self._data = np.array(self._data)\n","    self._labels = np.array(self._labels)\n","    self._sentence_lengths = np.array(self._sentence_lengths)\n","    self._final_tokens = np.array(self._final_tokens)\n","\n","    self._num_epoch = 0\n","    self._batch_id = 0\n","    self._size = len(self._data)\n","\n","  def next_batch(self):\n","    start = self._batch_id * self._batch_size\n","    end = start + self._batch_size\n","    self._batch_id += 1\n","\n","    if end + self._batch_size > len(self._data):\n","      self._size = end\n","      end = len(self._data)\n","      start = end - self._batch_size\n","      self._num_epoch += 1\n","      self._batch_id = 0\n","      indices = list(range(len(self._data)))\n","      random.seed(2021)\n","      random.shuffle(indices)\n","      self._data, self._labels, self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n","\n","    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]"]},{"cell_type":"markdown","metadata":{"id":"5JJdVNqUbARw"},"source":["# Build Model"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1679629258087,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"xIW7k7-6a_aS"},"outputs":[],"source":["MAX_DOC_LENGTH = 500\n","NUM_CLASSES = 20"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679629258088,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"xL5eQb7YMCLL"},"outputs":[],"source":["import numpy as np\n","\n","MAX_SENTENCE_LENGTH = 500\n","NUM_CLASSES = 20\n","\n","class RNN:\n","  def __init__(self,\n","               vocab_size,\n","               embedding_size,\n","               lstm_size,\n","               batch_size):\n","    self._vocab_size = vocab_size\n","    self._embedding_size = embedding_size\n","    self._lstm_size = lstm_size\n","    self._batch_size = batch_size\n","\n","    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_SENTENCE_LENGTH])\n","    self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n","    self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n","    self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n","\n","\n","  def embedding_layer(self, indices):\n","    pretrained_vectors = []\n","    pretrained_vectors.append(np.zeros(self._embedding_size))\n","    np.random.seed(2021)\n","    for _ in range (self._vocab_size + 1):\n","      pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n","\n","    pretrained_vectors = np.array(pretrained_vectors)\n","\n","    self._embedding_matrix = tf.get_variable(\n","        name='embedding',\n","        shape=(self._vocab_size + 2, self._embedding_size),\n","        initializer=tf.constant_initializer(pretrained_vectors)\n","    )\n","    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n","\n","\n","  def LSTM_layer(self, embeddings):\n","    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n","    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n","    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n","\n","    lstm_inputs = tf.unstack(\n","        tf.transpose(embeddings, perm=[1, 0, 2])\n","    )\n","    lstm_outputs, last_state = tf.nn.static_rnn(\n","        cell=lstm_cell,\n","        inputs=lstm_inputs,\n","        initial_state=initial_state,\n","        sequence_length=self._sentence_lengths\n","    )\n","    lstm_outputs = tf.unstack(\n","        tf.transpose(lstm_outputs, perm=[1, 0, 2])\n","    )\n","    lstm_outputs = tf.concat(\n","        lstm_outputs,\n","        axis=0\n","    )\n","    mask = tf.sequence_mask(\n","        lengths=self._sentence_lengths,\n","        maxlen=MAX_SENTENCE_LENGTH,\n","        dtype=tf.float32\n","    ) \n","    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n","    mask = tf.expand_dims(mask, -1)\n","\n","    lstm_outputs = mask * lstm_outputs\n","    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n","    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n","    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n","        tf.cast(self._sentence_lengths, tf.float32),\n","        -1\n","    ) \n","\n","    return lstm_outputs_average\n","\n","  def build_graph(self):\n","    embeddings = self.embedding_layer(self._data)\n","    lstm_outputs = self.LSTM_layer(embeddings)\n","\n","    weigths = tf.get_variable(\n","        name = 'final_layer_weights',\n","        shape = (self._lstm_size, NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed = 2021)\n","    )\n","    biases = tf.get_variable(\n","        name = 'final_layer_biases',\n","        shape = (NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed = 2021)\n","    )\n","    logits = tf.matmul(lstm_outputs, weigths) + biases\n","\n","    labels_one_hot = tf.one_hot(\n","        indices = self._labels,\n","        depth = NUM_CLASSES,\n","        dtype = tf.float32\n","    )\n","\n","    loss = tf.nn.softmax_cross_entropy_with_logits(\n","        labels = labels_one_hot,\n","        logits = logits\n","    )\n","    loss = tf.reduce_mean(loss)\n","\n","    probs = tf.nn.softmax(logits)\n","    predicted_labels = tf.argmax(probs, axis = 1)\n","    predicted_labels = tf.squeeze(predicted_labels)\n","    return predicted_labels, loss\n","\n","\n","  def trainer(self, loss, learning_rate):\n","    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","    return train_op"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1679629258088,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"grut_2albHuq"},"outputs":[],"source":["loss_report = []\n","accuracy_report = []\n","\n","def train_and_evaluate_RNN():\n","    with open(os.path.join(\"w2v\", \"vocab-raw.txt\")) as f:\n","      vocab_size = len(f.read().splitlines())\n","\n","    tf.set_random_seed(2023)\n","    rnn = RNN(\n","      vocab_size=vocab_size,\n","      embedding_size=300,\n","      lstm_size=50,\n","      batch_size=50\n","    )\n","    predicted_labels, loss = rnn.build_graph()\n","    train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n","\n","    with tf.Session() as sess:\n","        train_data_reader = DataReader(\n","            data_path=os.path.join(\"w2v\", \"20news-train-encoded.txt\"),\n","            batch_size=50,\n","        )\n","        test_data_reader = DataReader(\n","            data_path=os.path.join(\"w2v\", \"20news-test-encoded.txt\"),\n","            batch_size=50,\n","        )\n","        step = 0\n","        MAX_STEP = 3000\n","\n","        sess.run(tf.global_variables_initializer())\n","        while step < MAX_STEP:\n","            next_train_batch = train_data_reader.next_batch()\n","            train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n","            plabels_eval, loss_eval, _ = sess.run(\n","                [predicted_labels, loss, train_op],\n","                feed_dict={\n","                    rnn._data: train_data,\n","                    rnn._labels: train_labels,\n","                    rnn._sentence_lengths: train_sentence_lengths,\n","                    rnn._final_tokens: train_final_tokens\n","                }\n","            )\n","            step += 1\n","            if step % 20 == 0:\n","              loss_report.append(loss_eval)\n","              print('loss: {}'.format(loss_eval))\n","            if train_data_reader._batch_id == 0:\n","              num_true_preds = 0\n","              while True:\n","                next_test_batch = test_data_reader.next_batch()\n","                test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n","                test_plabels_eval = sess.run(\n","                    predicted_labels,\n","                    feed_dict={\n","                        rnn._data: test_data,\n","                        rnn._labels: test_labels,\n","                        rnn._sentence_lengths: test_sentence_lengths,\n","                        rnn._final_tokens: test_final_tokens\n","                    }\n","                )\n","                matches = np.equal(test_plabels_eval, test_labels)\n","                num_true_preds += np.sum(matches.astype(float))\n","\n","                if test_data_reader._batch_id == 0:\n","                  break\n","\n","              accuracy_report.append(num_true_preds * 100. / test_data_reader._size)        \n","              print('Epoch: {}'.format(train_data_reader._num_epoch))\n","              print('Accuracy on test data: {}'.format(num_true_preds * 100. / test_data_reader._size))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2153004,"status":"error","timestamp":1679631411086,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"hD-6kbE6bIxL","outputId":"a5f32402-0b98-4e27-f5d1-bc144d68eac8"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-7-866044ca4ad3>:41: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n","  lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n","WARNING:tensorflow:From <ipython-input-7-866044ca4ad3>:48: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py:797: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"]},{"name":"stdout","output_type":"stream","text":["loss: 0.04306371510028839\n","loss: 0.19866229593753815\n","loss: 6.1644978523254395\n","loss: 0.06579981744289398\n","loss: 3.7183914184570312\n","loss: 6.164046764373779\n","loss: 2.5654726028442383\n","loss: 2.0115537643432617\n","loss: 5.74612283706665\n","loss: 4.7553815841674805\n","loss: 4.03902006149292\n","Epoch: 1\n","Accuracy on test data: 8.146666666666667\n","loss: 3.148702383041382\n","loss: 2.43094539642334\n","loss: 2.008957862854004\n","loss: 1.8502798080444336\n","loss: 2.0781471729278564\n","loss: 1.8981690406799316\n","loss: 1.6476187705993652\n","loss: 1.3822146654129028\n","loss: 1.4513250589370728\n","loss: 1.5456435680389404\n","loss: 1.5562950372695923\n","Epoch: 2\n","Accuracy on test data: 69.56\n","loss: 1.0617033243179321\n","loss: 1.012721061706543\n","loss: 1.0230969190597534\n","loss: 0.8993005156517029\n","loss: 0.9783300161361694\n","loss: 0.7043697834014893\n","loss: 0.7388078570365906\n","loss: 0.6403293609619141\n","loss: 0.7412493228912354\n","loss: 0.5069388747215271\n","loss: 0.498835951089859\n","Epoch: 3\n","Accuracy on test data: 74.77333333333333\n","loss: 0.3378704786300659\n","loss: 0.15557488799095154\n","loss: 0.2899985611438751\n","loss: 0.2900727689266205\n","loss: 0.1474786251783371\n","loss: 0.0993231013417244\n","loss: 0.19090153276920319\n","loss: 0.16103941202163696\n","loss: 0.1360653042793274\n","loss: 0.2463439404964447\n","loss: 0.2164638638496399\n","loss: 0.37217476963996887\n","Epoch: 4\n","Accuracy on test data: 76.28\n","loss: 0.052581287920475006\n","loss: 0.040513940155506134\n","loss: 0.07040295749902725\n","loss: 0.09318216145038605\n","loss: 0.043077416718006134\n","loss: 0.040066562592983246\n","loss: 0.13224242627620697\n","loss: 0.037753619253635406\n","loss: 0.07596448808908463\n","loss: 0.0427684560418129\n","loss: 0.14339284598827362\n","Epoch: 5\n","Accuracy on test data: 76.57333333333334\n","loss: 0.02469806745648384\n","loss: 0.016510413959622383\n","loss: 0.016268128529191017\n","loss: 0.013859561644494534\n","loss: 0.012884114868938923\n","loss: 0.014990879222750664\n","loss: 0.007629436906427145\n","loss: 0.011950804851949215\n","loss: 0.018883077427744865\n","loss: 0.036345310509204865\n","loss: 0.017015870660543442\n","Epoch: 6\n","Accuracy on test data: 77.22666666666667\n","loss: 0.0049272868782281876\n","loss: 0.0036669387482106686\n","loss: 0.004594188649207354\n","loss: 0.004322105087339878\n","loss: 0.005063529592007399\n","loss: 0.0036534888204187155\n","loss: 0.0036112400703132153\n","loss: 0.003815353149548173\n","loss: 0.004376852419227362\n","loss: 0.0037419709842652082\n","loss: 0.005853199865669012\n","loss: 0.0033894493244588375\n","Epoch: 7\n","Accuracy on test data: 77.13333333333334\n","loss: 0.002653922187164426\n","loss: 0.002865472109988332\n","loss: 0.002702664351090789\n","loss: 0.002448601182550192\n","loss: 0.0030265836976468563\n","loss: 0.0022701642010360956\n","loss: 0.0022474664729088545\n","loss: 0.004327576607465744\n","loss: 0.003374091349542141\n","loss: 0.002650682581588626\n","loss: 0.003369635436683893\n","Epoch: 8\n","Accuracy on test data: 76.92\n","loss: 0.0011134697124361992\n","loss: 0.0027298086788505316\n","loss: 0.0019686257001012564\n","loss: 0.0012238079216331244\n","loss: 0.001526063191704452\n","loss: 0.0012430241331458092\n","loss: 0.0026580411940813065\n","loss: 0.0014338803011924028\n","loss: 0.0016612010076642036\n","loss: 0.0012497766874730587\n","loss: 0.0012768203159794211\n","Epoch: 9\n","Accuracy on test data: 76.97333333333333\n","loss: 0.002060909057036042\n","loss: 0.0013678596587851644\n","loss: 0.001074690488167107\n","loss: 0.0010994288604706526\n","loss: 0.0008069035247899592\n","loss: 0.0010346992639824748\n","loss: 0.000955010938923806\n","loss: 0.0009225454996339977\n","loss: 0.0008985892636701465\n","loss: 0.0014653514372184873\n","loss: 0.0011977533577010036\n","loss: 0.0009174626902677119\n","Epoch: 10\n","Accuracy on test data: 76.69333333333333\n","loss: 0.0008198028663173318\n","loss: 0.0007185895810835063\n","loss: 0.0006406416068784893\n","loss: 0.0007260253769345582\n","loss: 0.0006591692799702287\n","loss: 0.0008215901325456798\n","loss: 0.006014171987771988\n","loss: 0.0007004017243161798\n","loss: 0.0006905665504746139\n","loss: 0.0006404643645510077\n","loss: 0.000815774139482528\n","Epoch: 11\n","Accuracy on test data: 76.78666666666666\n","loss: 0.0005750245763920248\n","loss: 0.0008180331205949187\n","loss: 0.0005776307079941034\n","loss: 0.0005537574179470539\n","loss: 0.0005107199540361762\n","loss: 0.0007915656315162778\n","loss: 0.0006270678131841123\n","loss: 0.0005832704482600093\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-b12e96ddcc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-b7d3249a2971>\u001b[0m in \u001b[0;36mtrain_and_evaluate_RNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnext_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sentence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_final_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_train_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             plabels_eval, loss_eval, _ = sess.run(\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 feed_dict={\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_and_evaluate_RNN()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1679631411087,"user":{"displayName":"Han Jimmy","userId":"15617845934533132621"},"user_tz":-420},"id":"wOXeJHlkbSA8"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "def gather_20newsgropups_data():\n",
    "    path = \"../datasets/20news-bydate/\"\n",
    "    train_dir = path + \"20news-bydate-train\"\n",
    "    test_dit = path + \"20news-bydate-test\"\n",
    "\n",
    "    newsgroup_list = [news_group for news_group in os.listdir(train_dir)]\n",
    "    newsgroup_list.sort()\n",
    "    return newsgroup_list\n",
    "newsgroup_list = gather_20newsgropups_data()\n",
    "print(newsgroup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../datasets/20news-bydate/stop_words.txt\") as f:\n",
    "    stop_words = (f.read().splitlines())\n",
    "    # print(stop_words)\n",
    "stemmer = PorterStemmer()\n",
    "def collect_data_from(parent_dir, newsgroup_list):\n",
    "    data = []\n",
    "    for group_id, newsgroup in enumerate (newsgroup_list):\n",
    "        label = group_id\n",
    "        dir_path = parent_dir + \"/\" + newsgroup + \"/\"\n",
    "        files = [(filename, dir_path + filename)\n",
    "                for filename in os.listdir(dir_path)\n",
    "                if os.path.isfile(dir_path + filename)]\n",
    "        files.sort()\n",
    "        for filename, filepath in files:\n",
    "            # print(filepath)\n",
    "            with open(filepath, errors = \"ignore\") as f:\n",
    "                text = f.read().lower()\n",
    "                words = [stemmer.stem(word)\n",
    "                        for word in re.split(\"\\W\", text)\n",
    "                        if word not in stop_words]\n",
    "\n",
    "                content = \" \".join(words)\n",
    "                assert len(content.splitlines()) == 1\n",
    "                data.append(str(label) + \"<fff>\" +\n",
    "                            filename + \"<fff>\" + content)\n",
    "    return data\n",
    "\n",
    "path = \"../datasets/20news-bydate/\"\n",
    "train_dir = path + \"20news-bydate-train\"\n",
    "test_dir = path + \"20news-bydate-test\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = collect_data_from(parent_dir= train_dir, newsgroup_list = newsgroup_list)\n",
    "test_data = collect_data_from(parent_dir= test_dir, newsgroup_list = newsgroup_list)\n",
    "full_data = train_data + test_data\n",
    "with open (\"../datasets/20news-bydate/20news-train-processed.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(train_data))\n",
    "with open (\"../datasets/20news-bydate/20news-test-processed.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(test_data))\n",
    "with open (\"../datasets/20news-bydate/20news-full-processed.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(full_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic knowledge of TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is scored between 0 and 1. The higher the numerical weight value, the rarer the term. The smaller the weight, the more common the term. \n",
    "\n",
    "- **TF (term frequency) example**\n",
    "\n",
    "The TF (term frequency) of a word is the frequency of a word (i.e., number of times it appears) in a document. When you know TF, you’re able to see if you’re using a term too much or too little. When a 100-word document contains the term “cat” 12 times, the TF for the word ‘cat’ is\n",
    "\n",
    "TFcat = 12/100 i.e. 0.12\n",
    "\n",
    "- **IDF (inverse document frequency) example**\n",
    "\n",
    "The IDF (inverse document frequency) of a word is the measure of how significant that term is in the whole corpus (a body of documents).\n",
    "\n",
    "Let’s say the size of the corpus is 10,000,000 million documents. If we assume there are 0.3 million documents that contain the term “cat”, then the IDF (i.e. log {DF}) is given by the total number of documents (10,000,000) divided by the number of documents containing the term “cat” (300,000).\n",
    "\n",
    "IDF (cat) = log (10,000,000/300,000) = 1.52\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we suppose the data in the train is combined by total files\n",
    "def generate_vocabulary(data_path):\n",
    "\n",
    "    # compute inverse document frequency\n",
    "    def compute_idf(df, corpus_size):\n",
    "        # df stands for document frequency\n",
    "        assert df > 0\n",
    "        return np.log10(corpus_size * 1. / df)\n",
    "        \n",
    "    with open (data_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \"\"\"\n",
    "    doc_count is the list with keys are distince word in the\n",
    "    vocabulary and the values is the number of document\n",
    "    containing that word in the corpus\n",
    "    \"\"\"\n",
    "    doc_count = defaultdict(int)\n",
    "    corpus_size = len(lines)\n",
    "\n",
    "    for line in lines:\n",
    "        features = line.split('<fff>')\n",
    "        text = features[-1]\n",
    "        words = list(set(text.split()))\n",
    "        for word in words:\n",
    "            doc_count[word] += 1\n",
    "    \"\"\"\n",
    "    words_idfs is list containing pairs of values - word and idf of that word\n",
    "    under condition that the frequency is larger than 10\n",
    "    \"\"\"\n",
    "    words_idfs = [(word, compute_idf(document_freq, corpus_size))\n",
    "                for word, document_freq in zip(doc_count.keys(), doc_count.values())\n",
    "                if document_freq > 10 and not word.isdigit()]\n",
    "    words_idfs.sort(key=lambda x: -x[1])\n",
    "    print(\"Vocabulary size {}\". format(len(words_idfs)))\n",
    "    with open('../datasets/20news-bydate/words_idfs.txt', \"w\") as f:\n",
    "        f.write(\"\\n\".join([word + \"<fff>\" + str(idf) for word, idf in words_idfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(data_path):\n",
    "    with open (data_path) as f:\n",
    "        word_idfs = [(line.split(\"<fff>\")[0], line.split(\"<fff>\")[1])  \n",
    "                    for line in f.read.splitlines()]\n",
    "\n",
    "        idfs = dict(word_idfs)\n",
    "        word_IDS = dict([(word, index) for index, (word, idfs) in enumerate(word_idfs)])\n",
    "    \n",
    "    with open (data_path) as f:\n",
    "        document = [\n",
    "            (int(line.split(\"<fff>\")[0]),\n",
    "            int(line.split(\"<fff>\")[1]),\n",
    "            line.split(\"<fff>\")[2])\n",
    "            for line in f.read().splitlines()]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 10309\n"
     ]
    }
   ],
   "source": [
    "generate_vocabulary(\"../datasets/20news-bydate/20news-train-processed.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another approach to implement TF-IDF more naively but consumes larger computational workload (and time complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "      'this is the first document',\n",
    "      'this document is the second document',\n",
    "      'and this is the third one',\n",
    "      'is this the first document',\n",
    " ]\n",
    "\n",
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    print(\"len of the corpus:\", N)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "        idf_dict[i]=(np.log((1+N)/(count+1)))+1\n",
    "    return idf_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of the corpus: 4\n",
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
      "{'and': 1.916290731874155, 'document': 1.2231435513142097, 'first': 1.5108256237659907, 'is': 1.0, 'one': 1.916290731874155, 'second': 1.916290731874155, 'the': 1.0, 'third': 1.916290731874155, 'this': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "      for x in whole_data:\n",
    "        for y in x.split():\n",
    "          if len(y)<2:\n",
    "            continue\n",
    "          unique_words.add(y)\n",
    "      unique_words = sorted(list(unique_words))\n",
    "      vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "      Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "Vocabulary, idf_of_vocabulary=fit(corpus)\n",
    "\n",
    "print(Vocabulary)\n",
    "print(idf_of_vocabulary)\n",
    "# 'this is the first document',\n",
    "# 'this document is the second document',\n",
    "# 'and this is the third one',\n",
    "# 'is this the first document',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (v3.9.0:9cf6752276, Oct  5 2020, 11:29:23) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

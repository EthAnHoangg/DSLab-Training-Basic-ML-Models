{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_20newsgropups_data():\n",
    "    path = \"../datasets/20news-bydate/\"\n",
    "    train_dir = path + \"20news-bydate-train\"\n",
    "    test_dir = path + \"20news-bydate-test\"\n",
    "\n",
    "    newsgroup_list = [news_group for news_group in os.listdir(train_dir)]\n",
    "    newsgroup_list.sort()\n",
    "    with open (\"../datasets/20news-bydate/stop_words.txt\") as f:\n",
    "        stop_words = (f.read().splitlines())\n",
    "    # print(stop_words)\n",
    "    stemmer = PorterStemmer()\n",
    "    def collect_data_from(parent_dir, newsgroup_list):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate (newsgroup_list):\n",
    "            label = group_id\n",
    "            dir_path = parent_dir + \"/\" + newsgroup + \"/\"\n",
    "            files = [(filename, dir_path + filename)\n",
    "                    for filename in os.listdir(dir_path)\n",
    "                    if os.path.isfile(dir_path + filename)]\n",
    "            files.sort()\n",
    "            for filename, filepath in files:\n",
    "                    # print(filepath)\n",
    "                    with open(filepath, errors = \"ignore\") as f:\n",
    "                        text = f.read().lower()\n",
    "                        words = [stemmer.stem(word)\n",
    "                                for word in re.split(\"\\W+\", text)\n",
    "                                if word not in stop_words]\n",
    "\n",
    "                        content = \" \".join(words)\n",
    "                        assert len(content.splitlines()) == 1\n",
    "                        data.append(str(label) + \"<fff>\" +\n",
    "                                    filename + \"<fff>\" + content)\n",
    "        return data\n",
    "    train_data = collect_data_from(parent_dir= train_dir, newsgroup_list = newsgroup_list)\n",
    "    test_data = collect_data_from(parent_dir= test_dir, newsgroup_list = newsgroup_list)\n",
    "    full_data = train_data + test_data\n",
    "    with open (\"../datasets/20news-bydate/20news-train-processed.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open (\"../datasets/20news-bydate/20news-test-processed.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "    with open (\"../datasets/20news-bydate/20news-full-processed.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(full_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_20newsgropups_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic knowledge of TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is scored between 0 and 1. The higher the numerical weight value, the rarer the term. The smaller the weight, the more common the term. \n",
    "\n",
    "- **TF (term frequency) example**\n",
    "\n",
    "The TF (term frequency) of a word is the frequency of a word (i.e., number of times it appears) in a document. When you know TF, you’re able to see if you’re using a term too much or too little. When a 100-word document contains the term “cat” 12 times, the TF for the word ‘cat’ is\n",
    "\n",
    "TFcat = 12/100 i.e. 0.12\n",
    "\n",
    "- **IDF (inverse document frequency) example**\n",
    "\n",
    "The IDF (inverse document frequency) of a word is the measure of how significant that term is in the whole corpus (a body of documents).\n",
    "\n",
    "Let’s say the size of the corpus is 10,000,000 million documents. If we assume there are 0.3 million documents that contain the term “cat”, then the IDF (i.e. log {DF}) is given by the total number of documents (10,000,000) divided by the number of documents containing the term “cat” (300,000).\n",
    "\n",
    "IDF (cat) = log (10,000,000/300,000) = 1.52\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we suppose the data in the train is combined by total files\n",
    "def generate_vocabulary(data_path):\n",
    "\n",
    "    # compute inverse document frequency\n",
    "    def compute_idf(df, corpus_size):\n",
    "        # df stands for document frequency\n",
    "        assert df > 0\n",
    "        return np.log10(corpus_size * 1. / df)\n",
    "        \n",
    "    with open (data_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \"\"\"\n",
    "    doc_count is the list with keys are distince word in the\n",
    "    vocabulary and the values is the number of document\n",
    "    containing that word in the corpus\n",
    "    \"\"\"\n",
    "    doc_count = defaultdict(int)\n",
    "    corpus_size = len(lines)\n",
    "\n",
    "    for line in lines:\n",
    "        features = line.split('<fff>')\n",
    "        text = features[-1]\n",
    "        words = list(set(text.split()))\n",
    "        for word in words:\n",
    "            doc_count[word] += 1\n",
    "    \"\"\"\n",
    "    words_idfs is list containing pairs of values - word and idf of that word\n",
    "    under condition that the frequency is larger than 10\n",
    "    \"\"\"\n",
    "    words_idfs = [(word, compute_idf(document_freq, corpus_size))\n",
    "                for word, document_freq in zip(doc_count.keys(), doc_count.values())\n",
    "                if document_freq > 10 and not word.isdigit()]\n",
    "    words_idfs.sort(key=lambda x: -x[1])\n",
    "    print(\"Vocabulary size {}\". format(len(words_idfs)))\n",
    "    with open('../datasets/20news-bydate/words_idfs.txt', \"w\") as f:\n",
    "        f.write(\"\\n\".join([word + \"<fff>\" + str(idf) for word, idf in words_idfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 14230\n"
     ]
    }
   ],
   "source": [
    "generate_vocabulary(\"../datasets/20news-bydate/20news-full-processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(data_path):\n",
    "    with open (\"../datasets/20news-bydate/words_idfs.txt\") as f:\n",
    "        word_idfs = [(line.split(\"<fff>\")[0], float(line.split(\"<fff>\")[1]))  \n",
    "                    for line in f.read().splitlines()]\n",
    "\n",
    "        idfs = dict(word_idfs)\n",
    "        word_IDs = dict([(word, index) for index, (word, idf) in enumerate(word_idfs)])\n",
    "    \n",
    "    with open (data_path) as f:\n",
    "        documents = [\n",
    "            (int(line.split(\"<fff>\")[0]),\n",
    "            int(line.split(\"<fff>\")[1]),\n",
    "            line.split(\"<fff>\")[2])\n",
    "            for line in f.read().splitlines()]\n",
    "        total_doc_num = len(documents)\n",
    "        \n",
    "    data_tf_idf = []\n",
    "    for i, document in enumerate(documents):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Processing {i}-th/{total_doc_num} documents\".format(i = i, total_doc_num = total_doc_num))\n",
    "        # unpack document\n",
    "        label, doc_id, text = document\n",
    "        words = [word for word in text.split() if word in idfs]\n",
    "        word_set = list(set(words))\n",
    "\n",
    "        max_term_freq = max([words.count(word) for word in word_set])\n",
    "        words_tf_idf = []\n",
    "        sum_squares = 0.0\n",
    "        \n",
    "        for word in word_set:\n",
    "            term_freq = words.count(word)\n",
    "\n",
    "            tf_idf_value = term_freq * 1. / max_term_freq * idfs[word]\n",
    "            words_tf_idf.append((word_IDs[word], tf_idf_value))\n",
    "            sum_squares = sum_squares + tf_idf_value ** 2\n",
    "        \n",
    "        words_tfidfs_normalized = [str(index)+\":\"+ str(tf_idf_value / np.sqrt(sum_squares))\n",
    "                                for index, tf_idf_value in words_tf_idf]\n",
    "        spare_rep = \" \".join(words_tfidfs_normalized)\n",
    "        data_tf_idf.append((label, doc_id, spare_rep))\n",
    "    with open('../datasets/20news-bydate/data_tf_idf.txt', \"w\") as f:\n",
    "        f.write(\"\\n\".join([str(label) + \"<fff>\" + str(doc_id) + \"<fff>\" + spare_rep \\\n",
    "                            for label, doc_id, spare_rep in data_tf_idf]))\n",
    "    print(\"Already calculated tf-idf values of {} documents in the corpus. File was written and saved\".format(total_doc_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0-th/18846 documents\n",
      "Processing 1000-th/18846 documents\n",
      "Processing 2000-th/18846 documents\n",
      "Processing 3000-th/18846 documents\n",
      "Processing 4000-th/18846 documents\n",
      "Processing 5000-th/18846 documents\n",
      "Processing 6000-th/18846 documents\n",
      "Processing 7000-th/18846 documents\n",
      "Processing 8000-th/18846 documents\n",
      "Processing 9000-th/18846 documents\n",
      "Processing 10000-th/18846 documents\n",
      "Processing 11000-th/18846 documents\n",
      "Processing 12000-th/18846 documents\n",
      "Processing 13000-th/18846 documents\n",
      "Processing 14000-th/18846 documents\n",
      "Processing 15000-th/18846 documents\n",
      "Processing 16000-th/18846 documents\n",
      "Processing 17000-th/18846 documents\n",
      "Processing 18000-th/18846 documents\n",
      "Already calculated tf-idf values of 18846 documents in the corpus. File was written and saved\n"
     ]
    }
   ],
   "source": [
    "get_tf_idf(\"../datasets/20news-bydate/20news-full-processed.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
